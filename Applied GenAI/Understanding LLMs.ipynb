{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1d461c",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FApplied+GenAI&file=Understanding+LLMs.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Understanding%20LLMs.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Understanding%20LLMs.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FApplied%2520GenAI%2FUnderstanding%2520LLMs.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Understanding%20LLMs.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Applied%20GenAI/Understanding%20LLMs.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%2520GenAI/Understanding%2520LLMs.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%2520GenAI/Understanding%2520LLMs.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%2520GenAI/Understanding%2520LLMs.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%2520GenAI/Understanding%2520LLMs.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e408ff-907f-4ef5-ae6a-37c755bc26d4",
   "metadata": {},
   "source": [
    "# Understanding LLMs\n",
    "\n",
    "A quick experiment to illustrate what an LLM is actually doing\n",
    "\n",
    "Take a well known passage of text, like the first paragraph from \"A Tale of Two Cities\" by Charles Dickens and Harvy Dunn, 1921:\n",
    "\n",
    ">It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way--in short, the period was so far like the present period that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n",
    "\n",
    "Imagine you are the author and have started writing this paragraph.  How likely is it to end up with these words?\n",
    "\n",
    "You start with 'It'.  That narrows down what comes next.  Given all of the text samples an LLM is trained on we can detect the most likely next word and it is 'was'.  \n",
    "Now start with 'It was'.  What comes next?\n",
    "\n",
    "How long before we start with enough text that the LLM can actually decide the mostly likely continuation is the exact same passage?  After as few as 5 words an LLM may recite the remainder of the paragrph as the sequence is that unique!\n",
    "\n",
    "This is because the LLM was trained, learned from, a large amount of text which certainly contained this very popular book that is also in the [public domain](https://www.loc.gov/item/05000749/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714f953-b232-4766-959d-c94f638ad308",
   "metadata": {
    "id": "od_UkDpvRmgD",
    "tags": []
   },
   "source": [
    "---\n",
    "## Colab Setup\n",
    "\n",
    "To run this notebook in Colab run the cells in this section.  Otherwise, skip this section.\n",
    "\n",
    "This cell will authenticate to GCP (follow prompts in the popup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49ba7fb-0211-43a5-939f-71e6213bf2b7",
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1683726184843,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8UO9FnqyKBlF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdd5820-a8d3-44d0-aaa1-a9722ec4ef73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68869,
     "status": "ok",
     "timestamp": 1683726253709,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "N98-KK7LRkjm",
    "outputId": "09ec5008-0def-4e1a-c349-c598ee752f78",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a Colab Environment\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    !gcloud config set project {PROJECT_ID}\n",
    "    print('Colab authorized to GCP')\n",
    "except Exception:\n",
    "    print('Not a Colab Environment')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329bf643-d031-4eca-9231-08e903d501bf",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1400e0e-d4f3-46bd-afef-5785e047339b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name, min_version)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform', '1.66.0')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "    elif len(package) == 3:\n",
    "        if importlib.metadata.version(package[0]) < package[2]:\n",
    "            print(f'updating package {package[1]}')\n",
    "            install = True\n",
    "            !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f6269-8099-4c3b-8282-797f5b3cc34f",
   "metadata": {},
   "source": [
    "### API Enablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "390bb3dd-8a57-46eb-9a70-84cc62ad20ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c313391-c3fa-41b1-80ca-d7bbd653adcb",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d420e6bb-c0e5-42f9-a777-7e32ac7a9719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "    IPython.display.display(IPython.display.Markdown(\"\"\"<div class=\\\"alert alert-block alert-warning\\\">\n",
    "        <b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. The previous cells do not need to be run again⚠️</b>\n",
    "        </div>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c33ebd4-be60-4812-af0b-00f295af62d5",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa0ebb-0473-42e1-97d4-6fa6ba9fae21",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf698e18-fede-457f-8f85-3dfe004d55b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f980027-e473-4cab-9cdf-96159cf4cf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908fd8d9-d1dc-4139-a31d-43aae57260c0",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6db806-6298-4b82-bb51-3821420cc246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from IPython.display import Markdown\n",
    "from google.cloud import aiplatform\n",
    "import vertexai.generative_models # for Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a94b2c4-da68-480e-8be7-4dddf13e3d00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.66.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34544c5-5d12-4720-829e-0776ca521a13",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d40fc646-06da-4592-897e-b5a1a2730cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertexai.init(project = PROJECT_ID, location = REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e96ca4-5383-47e1-ad12-1fe39bd1a4a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup LLM and Predictor Function\n",
    "\n",
    "Vertex AI hosts many [Google models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) as an API.  It also host partner models and offers managed services for hosting private models as well as training models with a full suite of [MLOps capabilities](../MLOps/readme.md).  This notebook uses the [Vertex AI Gemini API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference) which is covered in much more depth in [this workflow](./Vertex%20AI%20Gemini%20API.ipynb).\n",
    "\n",
    "\n",
    "Connect to the `gemini-1.5-flash-001` model for text generation.  Then build a helper function around the LLM prediction calls to:\n",
    "- specify how many input words from the known `sample` to pass as input\n",
    "- specify how many output words to ask for\n",
    "- specify how many tries/request to make for output words\n",
    "    - adjust temperature from 0 to 0.1 to see more than just the most likely next word chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262cc7fa-4506-482d-a844-2172d63727a3",
   "metadata": {},
   "source": [
    "### Connect To Model and Sample Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a070b10-490f-4ab0-a2f9-124a85a962fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = vertexai.generative_models.GenerativeModel(\"gemini-1.5-flash-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ca49987-e7d2-454e-bba0-ef8d02e2f1f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The opening paragraph of *A Tale of Two Cities* is:\n",
       "\n",
       "> It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way—in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm.generate_content('What is the first paragraph of the tale of two cities?').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484c941-6bd1-4b5e-944b-090b2736a996",
   "metadata": {},
   "source": [
    "### Helper Function: Predictor\n",
    "\n",
    "Create a function that take 4 inputs:\n",
    "- `words` = the number of words provided to the llm\n",
    "    - innt\n",
    "- `new_words` = the number of words the llm should extend the input `words` by\n",
    "    - int\n",
    "- `trys` = how many response to generate\n",
    "    - int, default = 1\n",
    "- `context` = provide (optional) context to the LLM when answering \n",
    "    - string, default = None\n",
    "    \n",
    "The function will print out a summary of the input and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2b81a4b0-b52e-4b0a-b180-ad7be311ddd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def predictor(words, new_words, trys = 1, context = None):\n",
    "    # data\n",
    "    sample = \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way--in short, the period was so far like the present period that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\"\n",
    "    spaces = [i for i, char in enumerate(sample) if char == ' ']\n",
    "    \n",
    "    # parameters\n",
    "    start = sample[0:spaces[words-1]]\n",
    "    correct = sample[spaces[words-1]:spaces[words+new_words-1]].strip()   \n",
    "    prompt = f\"I am going to provide the start of a sentence and I want you to provide the next {new_words} words without repeating what I provide.\\n\\n{start}\"\n",
    "    \n",
    "    # LLM call\n",
    "    responses = await asyncio.gather(*[\n",
    "        llm.generate_content_async(\n",
    "            list(filter(None, (context, prompt))),\n",
    "            generation_config = vertexai.generative_models.GenerationConfig(\n",
    "                temperature = (0.5 if trys > 1 else 0),\n",
    "                top_p = 1,\n",
    "                top_k = new_words,\n",
    "                max_output_tokens = new_words*2,\n",
    "                seed = (42 + i)\n",
    "            )\n",
    "        ) for i in range(trys)\n",
    "    ])\n",
    "    \n",
    "    # text processing for comparisons: lowercase, remove spaces, remove punctuation\n",
    "    def process_string(text):\n",
    "        import string\n",
    "        text = text.strip().lower()\n",
    "        allowed_chars = string.ascii_lowercase + ' '\n",
    "        text = ''.join(c for c in text if c in allowed_chars)\n",
    "        return text\n",
    "    \n",
    "    # prepare result string:\n",
    "    result = ''#'**Results:**'\n",
    "    result += f'\\n- **Input:** {start}'\n",
    "    result += f'\\n- **Correct Response:** {correct}'\n",
    "    if trys == 1:\n",
    "        result += f'\\n- **Received Response:** {responses[0].text}'\n",
    "        if process_string(correct).startswith(process_string(responses[0].text)):\n",
    "            result += '\\n- **Result:** Correct'\n",
    "        else:\n",
    "            result += '\\n- **Result:** Incorrect'\n",
    "    else:\n",
    "        result += f'\\n- **Received Responses:**'\n",
    "        for response in responses:\n",
    "            result += f'\\n\\t- {response.text}'\n",
    "            \n",
    "    return Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "14b8a19b-4cc8-45cd-8f0e-3cc37a84cb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It\n",
       "- **Correct Response:** was\n",
       "- **Received Response:** is \n",
       "- **Result:** Incorrect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(1,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709556c-585f-4fd0-95a2-e2b30b52abc8",
   "metadata": {},
   "source": [
    "---\n",
    "## Predict the Next Word(s)...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bfb5ac-c0b3-48fe-a43f-2a4cd2a19ebe",
   "metadata": {},
   "source": [
    "Pass 1 word in, ask for 1 word out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5576c7b4-e9ca-4037-b741-2fdf3f06f5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It\n",
       "- **Correct Response:** was\n",
       "- **Received Response:** is\n",
       "- **Result:** Incorrect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0aefec-c8f0-4553-9097-b8ffd1e13ce0",
   "metadata": {},
   "source": [
    "Pass 1 word in, ask for 2 words out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3fe4f8e3-750e-4a4b-81f8-40585b7b1d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It\n",
       "- **Correct Response:** was the\n",
       "- **Received Response:** was a\n",
       "- **Result:** Incorrect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(1, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ccf17-7ccf-4195-a6bb-01bc7edc33e6",
   "metadata": {},
   "source": [
    "Pass 1 word in, ask for 2 words out, try 10 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be323173-4660-4584-91c8-2f6055842495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It\n",
       "- **Correct Response:** was the\n",
       "- **Received Responses:**\n",
       "\t- was raining\n",
       "\t- is a\n",
       "\t- was a\n",
       "\t- is a\n",
       "\t- is a\n",
       "\t- was a\n",
       "\t- is a\n",
       "\t- is a\n",
       "\t- is a\n",
       "\t- is a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(1, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9b171-5499-4713-b12d-d4e1bf1f99bf",
   "metadata": {},
   "source": [
    "Pass 1 word in, ask for 3 words out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "856c0f03-8822-4103-95ac-70ef482b9085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It\n",
       "- **Correct Response:** was the best\n",
       "- **Received Response:** was a dark\n",
       "- **Result:** Incorrect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(1, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694311a-eb8c-432d-830a-f3836f7bd9a6",
   "metadata": {},
   "source": [
    "Pass 1 word in, ask for 3 words out, try 10 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29ac4659-b67f-4770-a634-cae7e2c27a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It\n",
       "- **Correct Response:** was the best\n",
       "- **Received Responses:**\n",
       "\t- was a dark\n",
       "\t- was a dark\n",
       "\t- was a dark\n",
       "\t- was a dark\n",
       "\t- was a dark\n",
       "\t- was a dark\n",
       "\t- was a dark\n",
       "\t- was a dark\n",
       "\t- was a dark\n",
       "\t- is a beautiful"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(1, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2df9a2-d1ab-4d5f-a099-518b39878ce0",
   "metadata": {},
   "source": [
    "---\n",
    "## How Many Input Words Are Needed To Correctly Reproduce The Sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd96057-b4fb-409f-9f18-acccc5faddcb",
   "metadata": {},
   "source": [
    "Pass 2 words in, ask for 1 word out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "74745993-17f6-4962-a2b2-9f0492d97b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was\n",
       "- **Correct Response:** the\n",
       "- **Received Response:** a\n",
       "- **Result:** Incorrect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(2, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c0d66-e397-4efa-bb85-b848124aa69a",
   "metadata": {},
   "source": [
    "Pass 3 words in, ask for 1 word out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "346ba61a-c241-41f7-bd41-621d0c9d2445",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was the\n",
       "- **Correct Response:** best\n",
       "- **Received Response:** best\n",
       "- **Result:** Correct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(3, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a6d30-d1aa-4528-a7ce-3635de5d3526",
   "metadata": {},
   "source": [
    "Pass 4 words in, ask for 1 word out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6cf054e7-ece0-4be3-b6af-91b25044a22b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was the best\n",
       "- **Correct Response:** of\n",
       "- **Received Response:** of\n",
       "- **Result:** Correct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(4, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5ddf6-bc94-4c36-8375-d18ca43ea026",
   "metadata": {},
   "source": [
    "Pass 5 words in, ask for 1 word out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5aac306c-8473-45c2-95ea-181a3a0fca03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was the best of\n",
       "- **Correct Response:** times,\n",
       "- **Received Response:** times\n",
       "- **Result:** Correct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(5, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e26bf-44e2-4af4-ba2d-bade1fcb94f9",
   "metadata": {},
   "source": [
    "Pass 5 words in, ask for 4 words out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe4403e3-59d2-44aa-a907-6f41ab33cfd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was the best of\n",
       "- **Correct Response:** times, it was the\n",
       "- **Received Response:** times, it was\n",
       "- **Result:** Correct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(5, 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a5be41-56d9-4204-b0e5-719ee1c1cd4d",
   "metadata": {},
   "source": [
    "Pass 5 words in, ask for 20 words out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ece54493-9880-41d7-8eea-967fd52599bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was the best of\n",
       "- **Correct Response:** times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it\n",
       "- **Received Response:** times, it was the worst of times, it was the age \n",
       "\n",
       "- **Result:** Correct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(5, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a323708-5602-4816-982a-bdbe9f50b5b9",
   "metadata": {},
   "source": [
    "Pass 5 words in, ask for 20 words out, try 5 time: answer is CORRECT consistantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f009ad3e-d527-49c6-82a8-baba8155367d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was the best of\n",
       "- **Correct Response:** times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it\n",
       "- **Received Responses:**\n",
       "\t- times, it was the worst of times, it was the age of \n",
       "\n",
       "\t- times, it was the worst of times, it was the age of \n",
       "\n",
       "\t- times, it was the worst of times, it was the age \n",
       "\n",
       "\t- times, it was the worst of times, it was the age \n",
       "\n",
       "\t- times, it was the worst of times, it was the age \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(5, 20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401fbb2-c747-4fef-bb8e-8a47a39d2698",
   "metadata": {},
   "source": [
    "---\n",
    "## Does Context Help?\n",
    "\n",
    "Add a context to the word generation might boost how quickly the llm responds with the correct words.  In this case the passage is the first paragraph of a famous movel by Charles Dickens.  Provide the authors name as context and review the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67cb22e-123e-404d-aeb4-db6968419c12",
   "metadata": {},
   "source": [
    "Pass 1 word in, ask for 1 word out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "08753803-53c3-4dfb-a37c-d1604db3a345",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It\n",
       "- **Correct Response:** was\n",
       "- **Received Response:** was\n",
       "- **Result:** Correct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(1, 1, 1, 'Dickens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71409f1b-eb0a-4a0d-b7d3-8f697b7634df",
   "metadata": {},
   "source": [
    "Pass 1 word in, ask for 2 words out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a9847ec2-a208-420e-b8a0-6c20d82007ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It\n",
       "- **Correct Response:** was the\n",
       "- **Received Response:** was a\n",
       "- **Result:** Incorrect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(1, 2, 1, 'Dickens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf73845-ada0-4217-9912-3893645a7be4",
   "metadata": {},
   "source": [
    "Pass 2 words in, ask for 2 words out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2fb2b679-5087-41cf-86ec-3d7d709f1990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was\n",
       "- **Correct Response:** the best\n",
       "- **Received Response:** a dark\n",
       "- **Result:** Incorrect"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(2, 2, 1, 'Dickens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cca5b-3410-463f-99b2-83501e9baf5e",
   "metadata": {},
   "source": [
    "Pass 3 words in, ask for 3 words out, try 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "30bb1666-9e13-44ed-9d62-ad227ee10100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was the\n",
       "- **Correct Response:** best of times,\n",
       "- **Received Response:** best of times\n",
       "- **Result:** Correct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(3, 3, 1, 'Dickens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7395126d-62e8-4ce3-997f-f0edca88abc7",
   "metadata": {},
   "source": [
    "Pass 3 words in, ask for 10 words out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d5e9c5b6-1e06-433d-9d7a-6ff5439e8712",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "- **Input:** It was the\n",
       "- **Correct Response:** best of times, it was the worst of times, it\n",
       "- **Received Response:** best of times, it was \n",
       "\n",
       "- **Result:** Correct"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await predictor(3, 10, 1, 'Dickens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1723087-34cb-4ff9-9512-e2f460926f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd3567-acfe-4877-8ab4-0bd138756fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff01e78-34c2-4cbf-ab9b-1ae85a925266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441528c-b049-452d-8fbb-349b8e5e193b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
