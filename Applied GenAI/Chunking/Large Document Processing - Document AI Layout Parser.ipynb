{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e7a067a",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FApplied+GenAI%2FChunking&file=Large+Document+Processing+-+Document+AI+Layout+Parser.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Chunking/Large%20Document%20Processing%20-%20Document%20AI%20Layout%20Parser.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Chunking/Large%20Document%20Processing%20-%20Document%20AI%20Layout%20Parser.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FApplied%2520GenAI%2FChunking%2FLarge%2520Document%2520Processing%2520-%2520Document%2520AI%2520Layout%2520Parser.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Chunking/Large%20Document%20Processing%20-%20Document%20AI%20Layout%20Parser.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Applied%20GenAI/Chunking/Large%20Document%20Processing%20-%20Document%20AI%20Layout%20Parser.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%2520GenAI/Chunking/Large%2520Document%2520Processing%2520-%2520Document%2520AI%2520Layout%2520Parser.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%2520GenAI/Chunking/Large%2520Document%2520Processing%2520-%2520Document%2520AI%2520Layout%2520Parser.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%2520GenAI/Chunking/Large%2520Document%2520Processing%2520-%2520Document%2520AI%2520Layout%2520Parser.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Applied%2520GenAI/Chunking/Large%2520Document%2520Processing%2520-%2520Document%2520AI%2520Layout%2520Parser.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d4d65-d466-4fc5-95d7-b6ae4193fa7e",
   "metadata": {},
   "source": [
    "# Large Document Processing - Document AI Layout Parser\n",
    "\n",
    "Creating chunks of text from documents to be used in [retrieval](../Retrieval/readme.md) systems is made simple with the [Layout Parser from Document AI](https://cloud.google.com/document-ai/docs/layout-parse-chunk). A companion workflow already shows many ways of request processing and handling responses: [Process Documents - Document AI Layout Parser](./Process%20Documents%20-%20Document%20AI%20Layout%20Parser.ipynb).  This workflow uses the techniques from this prior workflow to process multiple very large documents.\n",
    "\n",
    "**Use Case Exploration**\n",
    "\n",
    "Buying a home usually involves borrowing money from a lending institution, typically through a mortgage secured by the home's value. But how do these institutions manage the risks associated with such large loans, and how are lending standards established?\n",
    "\n",
    "In the United States, two government-sponsored enterprises (GSEs) play a vital role in the housing market:\n",
    "- Federal National Mortgage Association ([Fannie Mae](https://www.fanniemae.com/))\n",
    "- Federal Home Loan Mortgage Corporation ([Freddie Mac](https://www.freddiemac.com/))\n",
    "\n",
    "These GSEs purchase mortgages from lenders, enabling those lenders to offer more loans. This process also allows Fannie Mae and Freddie Mac to set standards for mortgages, ensuring they are responsible and borrowers are more likely to repay them. This system makes homeownership more affordable and stabilizes the housing market by maintaining a steady flow of liquidity for lenders and keeping interest rates controlled.\n",
    "\n",
    "However, navigating the complexities of these GSEs and their extensive servicing guides can be challenging.\n",
    "\n",
    "**Approaches**\n",
    "\n",
    "[This series](../readme.md) covers many generative AI workflows.  These documents are directly used as long context for Gemini in the workflow [Long Context Retrieval With The Vertex AI Gemini API](../Generate/Long%20Context%20Retrieval%20With%20The%20Vertex%20AI%20Gemini%20API.ipynb).  The workflow below uses chunking of the document to set up a retrieval workflow.  The [Vertex AI Text Embeddings API](../Embeddings/Vertex%20AI%20Text%20Embeddings%20API.ipynb) workflow generates embeddings for these chunks that are then used throughout the [Retrieval](../Retrieval/readme.md) examples.\n",
    "\n",
    "## Costs\n",
    "It is **not recommended** to run this notebook as a tutorial as it processes several thousand pdf pages and cost around $40.  The outputs are saved in the repository with this notebook for review.  The code here could be considered as a helpful getting started guide for processing large documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd8c0f-1263-4d2c-897d-da379082db9c",
   "metadata": {
    "id": "od_UkDpvRmgD"
   },
   "source": [
    "---\n",
    "## Colab Setup\n",
    "\n",
    "When running this notebook in [Colab](https://colab.google/) or [Colab Enterprise](https://cloud.google.com/colab/docs/introduction), this section will authenticate to GCP (follow prompts in the popup) and set the current project for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473f4588-05a3-4695-9b83-d2e31f87cc3f",
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1683726184843,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8UO9FnqyKBlF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'statmike-mlops-349915' # replace with project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a26a4d-bac7-4b24-95d9-7ac11ab338b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68869,
     "status": "ok",
     "timestamp": 1683726253709,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "N98-KK7LRkjm",
    "outputId": "09ec5008-0def-4e1a-c349-c598ee752f78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    !gcloud config set project {PROJECT_ID}\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682da0f-2265-4b5c-b06d-d1f1eaa13f73",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs and API Enablement\n",
    "\n",
    "The clients packages may need installing in this environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc3b02-6d6c-468b-9220-d2c0e3d4bf42",
   "metadata": {},
   "source": [
    "### Installs (If Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab05ebe-64a7-420e-93b9-442a60bfb31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name, min_version)\n",
    "packages = [\n",
    "    ('google.cloud.documentai', 'google-cloud-documentai', '2.31.0'),\n",
    "    ('google.cloud.storage', 'google-cloud-storage'),\n",
    "    ('fitz', 'pymupdf'),\n",
    "    ('PIL', 'Pillow')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "    elif len(package) == 3:\n",
    "        if importlib.metadata.version(package[0]) < package[2]:\n",
    "            print(f'updating package {package[1]}')\n",
    "            install = True\n",
    "            !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015901fd-9416-408f-90e0-46d06548aad3",
   "metadata": {},
   "source": [
    "### API Enablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a63feac-366c-4a69-b29e-ffad1972bdb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud services enable documentai.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd262268-39fc-4645-b5c6-58e5d1d0c299",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28c1fbf-875e-48c0-a602-833c4148b341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "    IPython.display.display(IPython.display.Markdown(\"\"\"<div class=\\\"alert alert-block alert-warning\\\">\n",
    "        <b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. The previous cells do not need to be run again⚠️</b>\n",
    "        </div>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a9f40-5431-43fc-910c-cf668e1b632f",
   "metadata": {
    "id": "appt8-yVRtJ1"
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9abf1-e932-49e2-9077-52989d90a237",
   "metadata": {
    "id": "63mx2EozRxFP"
   },
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf3cfd7-b44b-4de8-9857-5af529d6ce44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2124,
     "status": "ok",
     "timestamp": 1683726390544,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "xzcoXjM5Rky5",
    "outputId": "b3bdcbc1-70d5-472e-aea2-42c74a42efde",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb56d6b0-cedc-44cc-95c0-cb75f0019191",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683726390712,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "IxWrFtqYMfku",
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "SERIES = 'applied-genai'\n",
    "EXPERIMENT = 'layout-parser-large-files'\n",
    "\n",
    "# make this the gcs bucket for storing files\n",
    "GCS_BUCKET = PROJECT_ID "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb14a15-e030-4e21-b56f-5c898def0143",
   "metadata": {
    "id": "LuajVwCiO6Yg"
   },
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f1c9f04-2a94-4e89-a069-2bbcaa592579",
   "metadata": {
    "executionInfo": {
     "elapsed": 17761,
     "status": "ok",
     "timestamp": 1683726409304,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "LVC7zzSLRk2C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, time, io, re, json\n",
    "\n",
    "import requests\n",
    "import fitz #pymupdf\n",
    "import PIL.Image\n",
    "\n",
    "from google.cloud import documentai\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb35ad-66c8-4299-ba43-7dd35145f5cd",
   "metadata": {
    "id": "EyAVFG9TO9H-"
   },
   "source": [
    "Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90886979-db44-4784-a898-c2c2a04723b2",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1683726409306,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "L0RPE13LOZce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# document AI client\n",
    "LOCATION = REGION.split('-')[0]\n",
    "docai_client = documentai.DocumentProcessorServiceClient(\n",
    "    client_options = dict(api_endpoint = f\"{LOCATION}-documentai.googleapis.com\")\n",
    ")\n",
    "docai_async_client = documentai.DocumentProcessorServiceAsyncClient(\n",
    "    client_options = dict(api_endpoint = f\"{LOCATION}-documentai.googleapis.com\")\n",
    ")\n",
    "\n",
    "# gcs client: assumes bucket already exists\n",
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "bucket = gcs.bucket(GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53b839-17b7-498f-9ac9-ef68f58c2987",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f01e2e-e450-481a-8961-52890bba7683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIR = f\"files/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97d924-0c85-4197-8edc-aef104b3933f",
   "metadata": {},
   "source": [
    "Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98255b1-7228-4a07-aac6-2f1330b04196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b41cb0-d41f-491c-a540-1476d1edac9b",
   "metadata": {},
   "source": [
    "---\n",
    "## Documents\n",
    "\n",
    "Retrieve the documents and store in GCS for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f12025-28c2-4356-addc-c4a8907b95b0",
   "metadata": {},
   "source": [
    "### Retrieve Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a97f8290-ee94-45bb-97dd-13d71e7c6fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freddie_url = 'https://guide.freddiemac.com/ci/okcsFattach/get/1002095_2'\n",
    "fannie_url = 'https://singlefamily.fanniemae.com/media/39861/display'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d41d247-e648-4ba4-b3b8-68098a2d1000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freddie_retrieve = requests.get(freddie_url).content\n",
    "fannie_retrieve = requests.get(fannie_url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe8891c-d99c-40df-aa52-962b9a0d8913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freddie_doc = fitz.open(stream = freddie_retrieve, filetype = 'pdf')\n",
    "fannie_doc = fitz.open(stream = fannie_retrieve, filetype = 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dedf602-cbd4-4da0-be68-c9ac534b5df2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2641, 1180)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freddie_doc.page_count, fannie_doc.page_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca1b00-36f0-40c5-9846-738e74baae27",
   "metadata": {},
   "source": [
    "### Split Documents\n",
    "\n",
    "The layout parser has a maximum page size per document of 500 pages and can handle 5,000 files.  Here the pdf is split into parts of no more than 400 pages.\n",
    "- [Layout Parser Limits](https://cloud.google.com/document-ai/docs/layout-parse-chunk#limitations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5078cea9-ad00-4ec1-affb-96d87a880a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def doc_parts(doc):\n",
    "    start_page = 0\n",
    "    max_pages = 400\n",
    "    n_pages = doc.page_count\n",
    "    \n",
    "    doc_list = []\n",
    "    while start_page < n_pages:\n",
    "        end_page = min(start_page + max_pages - 1, n_pages)\n",
    "        new_doc = fitz.open()\n",
    "        new_doc.insert_pdf(doc, from_page = start_page, to_page = end_page)\n",
    "        doc_list.append(new_doc)\n",
    "        start_page = end_page + 1\n",
    "    \n",
    "    print(f\"The document has {n_pages} pages and has been split into parts with page counts: {[p.page_count for p in doc_list]}\")\n",
    "    \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b387b7df-03f2-44ba-991b-a100073d1f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document has 2641 pages and has been split into parts with page counts: [400, 400, 400, 400, 400, 400, 241]\n"
     ]
    }
   ],
   "source": [
    "freddie_parts = doc_parts(freddie_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "863726f7-f458-45cd-bdf9-eded1eccdcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document has 1180 pages and has been split into parts with page counts: [400, 400, 380]\n"
     ]
    }
   ],
   "source": [
    "fannie_parts = doc_parts(fannie_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc521ea5-af2e-4ae9-85b2-e7255ed1a016",
   "metadata": {},
   "source": [
    "### Save Documents To GCS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ee97186-80da-40c2-b971-e69e30f7c535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def doc_to_gcs(document, name):\n",
    "    buffer = io.BytesIO()\n",
    "    document.save(buffer)\n",
    "    buffer.seek(0) # reset the position to the beginning\n",
    "    blob = bucket.blob(f\"{SERIES}/{EXPERIMENT}/{name}.pdf\")\n",
    "    blob.upload_from_file(buffer, content_type = 'application/pdf')\n",
    "    print(f\"The file 'gs://{bucket.name}/{blob.name}' is {(blob.size / (1024*1024)):.2f} MB\")\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ffe5b7c-1a92-4b31-9b92-6c0191d951f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/full/freddie.pdf' is 21.44 MB\n"
     ]
    }
   ],
   "source": [
    "freddie_blob = doc_to_gcs(freddie_doc, 'full/freddie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd47c2ac-a3eb-4c8d-831d-9edaef122888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/full/fannie.pdf' is 4.55 MB\n"
     ]
    }
   ],
   "source": [
    "fannie_blob = doc_to_gcs(fannie_doc, 'full/fannie')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9fec80-f9a7-4f37-82f1-08d387b1b496",
   "metadata": {},
   "source": [
    "### Save Document Parts To GCS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9db31803-d10d-4616-b976-8807f93c4e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/freddie_part_0.pdf' is 3.17 MB\n",
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/freddie_part_1.pdf' is 4.44 MB\n",
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/freddie_part_2.pdf' is 3.32 MB\n",
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/freddie_part_3.pdf' is 3.43 MB\n",
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/freddie_part_4.pdf' is 3.38 MB\n",
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/freddie_part_5.pdf' is 2.89 MB\n",
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/freddie_part_6.pdf' is 2.22 MB\n"
     ]
    }
   ],
   "source": [
    "freddie_blobs = [doc_to_gcs(doc, f'parts/freddie_part_{d}') for d, doc in enumerate(freddie_parts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7c617df-3474-451b-8713-726414b8b58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/fannie_part_0.pdf' is 1.43 MB\n",
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/fannie_part_1.pdf' is 1.40 MB\n",
      "The file 'gs://statmike-mlops-349915/applied-genai/layout-parser-large-files/parts/fannie_part_2.pdf' is 1.23 MB\n"
     ]
    }
   ],
   "source": [
    "fannie_blobs = [doc_to_gcs(doc, f'parts/fannie_part_{d}') for d, doc in enumerate(fannie_parts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d959e3f-b1c7-4468-8e99-3cf88675f785",
   "metadata": {},
   "source": [
    "---\n",
    "## Layout Parser\n",
    "\n",
    "Document AI is comprised of multiple processors.  In this case the [Layout Parser](https://cloud.google.com/document-ai/docs/layout-parse-chunk) is used for its ability to detect and extract paragraphs, tables, titles, heading, page headers, and page footers.  \n",
    "\n",
    "For a more thorough review of Document AI processors, including customized parsers, see the [Working With/Document AI](../../Working%20With/Document%20AI/readme.md) section of this repository.  This repository includes examples of processing documents at larger scales and storing the data for processing and retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20704fff-1c9a-4ad9-a2bc-194f0256de55",
   "metadata": {
    "id": "Ydo-up2TMwo1"
   },
   "source": [
    "### Get/Create Processor: Layout Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e06a4cb-44e7-4e45-8a40-2a334f719fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved existing parser:  projects/1026793852137/locations/us/processors/3779bd3a8f535977\n"
     ]
    }
   ],
   "source": [
    "PARSER_DISPLAY_NAME = 'my_layout_processor'\n",
    "PARSER_TYPE = 'LAYOUT_PARSER_PROCESSOR'\n",
    "PARSER_VERSION = 'pretrained-layout-parser-v1.0-2024-06-03'\n",
    "\n",
    "for p in docai_client.list_processors(parent = f'projects/{PROJECT_ID}/locations/{LOCATION}'):\n",
    "    if p.display_name == PARSER_DISPLAY_NAME:\n",
    "        parser = p\n",
    "try:\n",
    "    print('Retrieved existing parser: ', parser.name)\n",
    "except Exception:\n",
    "    parser = docai_client.create_processor(\n",
    "        parent = f'projects/{PROJECT_ID}/locations/{LOCATION}',\n",
    "        processor = dict(display_name = PARSER_DISPLAY_NAME, type_ = PARSER_TYPE, default_processor_version = PARSER_VERSION)\n",
    "    )\n",
    "    print('Created New Parser: ', parser.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730f829-e49e-4dfd-baff-5b9b59e099a9",
   "metadata": {},
   "source": [
    "---\n",
    "## Process Documents\n",
    "\n",
    "For a complete overview of online and batch processing options check out the companion workflow: [Process Documents - Document AI Layout Parser](./Process%20Documents%20-%20Document%20AI%20Layout%20Parser.ipynb).  Here batch processing is used to accomodate the file size and number of files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd81bdc-65ad-4cfe-b2aa-d05df0226f81",
   "metadata": {},
   "source": [
    "### Batch Processing: Multiple Documents and/or Larger Documents\n",
    "\n",
    "With batch processing there are two ways to specify documents.  A list of documents with uris or a prefix for the uri to match.  Either of these would work for the `input_documents` parameter of batch processing here:\n",
    "\n",
    "**List Each Document**\n",
    "```\n",
    "        input_documents = documentai.BatchDocumentsInputConfig(\n",
    "            gcs_documents = documentai.GcsDocuments(\n",
    "                documents = [\n",
    "                    \n",
    "                    documentai.GcsDocument(\n",
    "                        gcs_uri = f'gs://{bucket.name}/{SERIES}/{EXPERIMENT}/files/document.pdf',\n",
    "                        mime_type = 'application/pdf'\n",
    "                    ),\n",
    "                    documentai.GcsDocument(\n",
    "                        gcs_uri = f'gs://{bucket.name}/{SERIES}/{EXPERIMENT}/files/small_document.pdf',\n",
    "                        mime_type = 'application/pdf'\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "```\n",
    "**Common Prefix For Documents**\n",
    "```\n",
    "        input_documents = documentai.BatchDocumentsInputConfig(\n",
    "            gcs_prefix = documentai.GcsPrefix(\n",
    "                gcs_uri_prefix = f'gs://{bucket.name}/{SERIES}/{EXPERIMENT}/files'\n",
    "            )\n",
    "        )\n",
    "```\n",
    "\n",
    "Reference:\n",
    "- [google.cloud.documentai.DocumentProcessorServiceClient.batch_process_documents()](https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.services.document_processor_service.DocumentProcessorServiceClient#google_cloud_documentai_v1_services_document_processor_service_DocumentProcessorServiceClient_batch_process_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2764bfe0-41c6-4d0c-bcf3-f3849133a93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = docai_client.batch_process_documents(\n",
    "    request = documentai.BatchProcessRequest(\n",
    "        name = parser.name,\n",
    "        input_documents = documentai.BatchDocumentsInputConfig(\n",
    "            gcs_prefix = documentai.GcsPrefix(\n",
    "                gcs_uri_prefix = f'gs://{bucket.name}/{SERIES}/{EXPERIMENT}/parts/'\n",
    "            )\n",
    "        ),\n",
    "        document_output_config = documentai.DocumentOutputConfig(\n",
    "            gcs_output_config = documentai.DocumentOutputConfig.GcsOutputConfig(\n",
    "                gcs_uri = f'gs://{bucket.name}/{SERIES}/{EXPERIMENT}/parsing/parts'\n",
    "            )\n",
    "        ),\n",
    "        process_options = documentai.ProcessOptions(\n",
    "            layout_config = documentai.ProcessOptions.LayoutConfig(\n",
    "                chunking_config = documentai.ProcessOptions.LayoutConfig.ChunkingConfig(\n",
    "                    chunk_size = 200,\n",
    "                    include_ancestor_headings = True,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0ebe3-7d95-47ee-894a-171e11b28c76",
   "metadata": {},
   "source": [
    "**NOTE:** This could take awhile (15-30 minutes). The next cell with continually check on the progress and hold up execution until the batch job is complete.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51dc6576-3c8e-4a9c-bc63-b17f343744e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on batch job to complete: projects/1026793852137/locations/us/operations/11360179740237196273\n",
      "State.SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "print(f'Waiting on batch job to complete: {batch_job.operation.name}')\n",
    "\n",
    "while batch_job.running():\n",
    "    time.sleep(10)\n",
    "\n",
    "batch_job.result()\n",
    "\n",
    "print(documentai.BatchProcessMetadata(batch_job.metadata).state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada715a5-b753-4ab7-b6b7-f3ceddd3f1e6",
   "metadata": {},
   "source": [
    "#### Retrieve Document Parsing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f6abc63-f68f-47ba-8644-efa6b618aca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_responses = []\n",
    "for process in documentai.BatchProcessMetadata(batch_job.metadata).individual_process_statuses:\n",
    "    matches = re.match(r\"gs://(.*?)/(.*)\", process.output_gcs_destination)\n",
    "    output_bucket, output_prefix = matches.groups()\n",
    "    output_blobs = bucket.list_blobs(prefix = output_prefix)\n",
    "    for blob in output_blobs:\n",
    "        response = documentai.Document.from_json(blob.download_as_bytes(), ignore_unknown_fields = True)\n",
    "        batch_responses.append((blob.name.split('/')[-1], response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "505319b8-97ea-47c4-9768-0ac657953771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d21c4-6008-4453-9379-bb3a6849e2f9",
   "metadata": {},
   "source": [
    "#### Review response for a document:\n",
    "\n",
    "The full contents of the response will be covered in the [Process Responses](#process-responses) section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12e2471f-4252-4cc9-88ee-0a157fc86b45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fannie_part_0-0.json', 'fannie_part_1-0.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_responses[0][0], batch_responses[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3d9e5fe-678b-4a1f-bc77-955b6519b9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chunk_id: \"c1\"\n",
       "content: \"Fannie Mae\"\n",
       "page_span {\n",
       "  page_start: 1\n",
       "  page_end: 1\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_responses[0][1].chunked_document.chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e1d1203-c9b0-44c8-a05f-2479c5ff5ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chunk_id: \"c1\"\n",
       "content: \"# (b) How to obtain a short sale property value and minimum net proceeds\\n\\nWith the exception of Mortgages secured by properties subject to resale restrictions (in accordance with Chapters 4406, 4502 or 4504, as applicable), the Servicer must submit a request to Freddie Mac for the short sale property value and the minimum net proceeds via the \\342\\200\\234Obtain Valuation\\342\\200\\235 tab in Freddie Mac Real Estate Valuation and Pricing tool when considering a Borrower for a short sale. The Servicer must advise the Borrower that the person evaluating the Mortgaged Premises must be given interior access and that the Borrower must otherwise cooperate with the inspection. An \\342\\200\\234estimated market value\\342\\200\\235 of the Mortgaged Premises and the \\342\\200\\234minimum net proceeds\\342\\200\\235 as determined by Freddie Mac will be returned by the Real Estate Valuation and Pricing tool with a \\\"good through date\\342\\200\\235 indicating the expiration date of the property value and minimum net proceeds amount. If the Servicer is unable to render an evaluation decision on a purchase offer prior to the good through date, a new property value and minimum net proceeds must be obtained via the Real Estate Valuation and Pricing tool to evaluate the purchase offer.\"\n",
       "page_span {\n",
       "  page_start: 1\n",
       "  page_end: 1\n",
       "}\n",
       "page_footers {\n",
       "  text: \"Freddie Mac Single-Family Seller/Servicer Guide As of 10/09/24\"\n",
       "  page_span {\n",
       "    page_start: 1\n",
       "    page_end: 1\n",
       "  }\n",
       "}\n",
       "page_footers {\n",
       "  text: \"Chapter 9208 Page 9208-29\"\n",
       "  page_span {\n",
       "    page_start: 1\n",
       "    page_end: 1\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_responses[-1][-1].chunked_document.chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbfe88-6a14-4dbb-b21d-2af5553cc054",
   "metadata": {},
   "source": [
    "---\n",
    "## Process Responses\n",
    "\n",
    "Create and save the chunks for further processing, like adding text embeddings with the workflow: [Vertex AI Text Embeddings API](../Embeddings/Vertex%20AI%20Text%20Embeddings%20API.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a81a26-d092-4140-8402-26e221de94f7",
   "metadata": {},
   "source": [
    "### Shape Data For Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adf13190-f88b-4d7f-98c1-7f4f37f9f254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = [\n",
    "    dict(\n",
    "        gse = batch[0].split('_')[0],\n",
    "        filename = batch[0].split('-')[0],\n",
    "        file_chunk_id = chunk.chunk_id,\n",
    "        chunk_id = batch[0].split('-')[0] + '_' + chunk.chunk_id,\n",
    "        content = chunk.content,\n",
    "    ) for batch in batch_responses for chunk in batch[1].chunked_document.chunks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f7c3668-eba4-4087-a644-77611d783771",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9040"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d23140e7-d401-4a37-97f2-498817ada8e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gse': 'fannie',\n",
       " 'filename': 'fannie_part_0',\n",
       " 'file_chunk_id': 'c1',\n",
       " 'chunk_id': 'fannie_part_0_c1',\n",
       " 'content': 'Fannie Mae'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e52040b5-3586-4844-98dc-bcb5210f24fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gse': 'freddie',\n",
       " 'filename': 'freddie_part_6',\n",
       " 'file_chunk_id': 'c618',\n",
       " 'chunk_id': 'freddie_part_6_c618',\n",
       " 'content': \"# 9701.23: Consent Agreement terms and conditions (10/09/24)\\n\\n## (c) Consent Agreements\\n\\nExhibit 33D, Acknowledgment Agreement (Combination) Incorporated Provisions, as applicable. In no event shall any Advance Financing be cross-collateralized with any Collateral under any Servicing Contract Rights Financing. Any collateral under any Advance Financing is and will continue to be at all times separate and distinct from any and all Collateral under any Servicing Contract Rights Financing.”\\n\\n# (d) Collateral Pledge Agreements\\n\\nFreddie Mac reserves the right to condition its entry into a Consent Agreement on the Servicer's pledge of collateral pursuant to a Collateral Pledge Agreement in substantially the form and substance of Exhibit 104, Collateral Pledge Agreement.\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1fd5a-17ba-42bf-ab47-bc361ed95d5e",
   "metadata": {},
   "source": [
    "### Save Data Locally\n",
    "\n",
    "Also, commit the files with this repository for future use by other workflows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c61f52a-2d63-4e6a-bf52-8db6da0b0729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9040"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fa7f78f-f722-476c-80ec-0dbdb6f21888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_chunk = 0\n",
    "max_chunk = 1000\n",
    "chunk_lists = []\n",
    "while start_chunk < len(chunks):\n",
    "    end_chunk = min(start_chunk + max_chunk, len(chunks))\n",
    "    chunk_lists.append(chunks[start_chunk:end_chunk])\n",
    "    start_chunk = end_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87b12337-97cf-46e3-ab02-f89505dbcaf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9040"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(c) for c in chunk_lists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1345f592-ec51-4991-8ec1-1c77bdcecc24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c, cl in enumerate(chunk_lists):\n",
    "    with open(f'{DIR}/document-chunks-{c:04d}.jsonl', 'w') as f:\n",
    "        for chunk in cl:\n",
    "            f.write(json.dumps(chunk)+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9aae54dd-eaec-4e1b-bb6a-d9d4cda6eb17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document-chunks-0008.jsonl',\n",
       " 'document-chunks-0002.jsonl',\n",
       " '.ipynb_checkpoints',\n",
       " 'document-chunks-0003.jsonl',\n",
       " 'document-chunks-0009.jsonl',\n",
       " 'document-chunks-0006.jsonl',\n",
       " 'document-chunks-0004.jsonl',\n",
       " 'document-chunks-0001.jsonl',\n",
       " 'document-chunks-0007.jsonl',\n",
       " 'document-chunks-0005.jsonl',\n",
       " 'document-chunks-0000.jsonl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DIR)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
