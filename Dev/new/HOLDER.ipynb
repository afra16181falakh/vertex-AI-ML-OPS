{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915e12af",
   "metadata": {},
   "source": [
    "![tracker](https://us-central1-vertex-ai-mlops-369716.cloudfunctions.net/pixel-tracking?path=statmike%2Fvertex-ai-mlops%2FDev%2Fnew&file=HOLDER.ipynb)\n",
    "<!--- header table --->\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/statmike/vertex-ai-mlops/blob/main/Dev/new/HOLDER.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\">\n",
    "      <br>View on<br>GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Dev/new/HOLDER.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\">\n",
    "      <br>Run in<br>Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fstatmike%2Fvertex-ai-mlops%2Fmain%2FDev%2Fnew%2FHOLDER.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\">\n",
    "      <br>Run in<br>Colab Enterprise\n",
    "    </a>\n",
    "  </td>      \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/statmike/vertex-ai-mlops/blob/main/Dev/new/HOLDER.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/bigquery/v1/32px.svg\" alt=\"BigQuery logo\">\n",
    "      <br>Open in<br>BigQuery Studio\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/statmike/vertex-ai-mlops/main/Dev/new/HOLDER.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\">\n",
    "      <br>Open in<br>Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Share This On: </b> \n",
    "    <a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/statmike/vertex-ai-mlops/blob/main/Dev%2Fnew/HOLDER.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://reddit.com/submit?url=https%3A//github.com/statmike/vertex-ai-mlops/blob/main/Dev%2Fnew/HOLDER.ipynb\"><img src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/statmike/vertex-ai-mlops/blob/main/Dev%2Fnew/HOLDER.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/statmike/vertex-ai-mlops/blob/main/Dev%2Fnew/HOLDER.ipynb\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a> \n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td colspan=\"5\" style=\"text-align: right\">\n",
    "    <b>Connect With Author On: </b> \n",
    "    <a href=\"https://www.linkedin.com/in/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"Linkedin Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://www.github.com/statmike\"><img src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://www.youtube.com/@statmike-channel\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fd/YouTube_full-color_icon_%282024%29.svg\" alt=\"YouTube Logo\" width=\"20px\"></a>\n",
    "    <a href=\"https://bsky.app/profile/statmike.bsky.social\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"BlueSky Logo\" width=\"20px\"></a> \n",
    "    <a href=\"https://x.com/statmike\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X (Twitter) Logo\" width=\"20px\"></a>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9f166-0b6f-4686-aee6-eb263f16a98f",
   "metadata": {},
   "source": [
    "## A HOLDING PLACE FOR CONTENT IN THIS FOLDER\n",
    "\n",
    "- goals:\n",
    "    - data inputs\n",
    "    - feature preprocess (before, at, inside)\n",
    "    - post processing, and modularity\n",
    "    - tensorboard\n",
    "    - saving and reusing, show some modularity here\n",
    "    - custom jogs\n",
    "    - experiment tracking\n",
    "    - model registry\n",
    "    - evals in model registry\n",
    "    - model monitoring for autoencoders\n",
    "    \n",
    "- dev:\n",
    "    - autoencoder architecture\n",
    "    - NAS\n",
    "    - Anomaly detection with\n",
    "    - contrastive learning\n",
    "    - VAE\n",
    "    - CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdaa58e-cb46-4ff8-9079-c10da0a2850e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac4bb5-02fa-4b82-98fc-f2a49b069162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c5eb5-2273-46c6-a571-9ffd68fe1ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8062607-e3e8-4fdc-9d14-4949e2f8af60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f358e77-5f53-4e15-9389-47c7f98fe080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59a594-a11f-4636-ae43-9bb918c717e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cbaf724-b467-43d3-bf07-1ecc20a4045b",
   "metadata": {},
   "source": [
    "### Training In TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64035f-a1a7-47c7-89f3-bcf229fb25e4",
   "metadata": {},
   "source": [
    "Build a normalization layer with [Keras Preprocessing Layers](https://www.tensorflow.org/guide/keras/preprocessing_layers). In this case all columns are already numeric so applying [tf.keras.layers.Normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) to the 'feature_array' column using the option `axis = -1` will calculate the mean and variance for each element of the 'feature_array' across all records.  This calculation only need to be done onece and can be triggered with the built in [.adapt()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization#adapt) method.  The `adapt()` method forces the calculation of the mean and variance across and input dataset.  In the case, we modify the `tf.data.Dataset` reader built above to only return the 'feature_array' column and then pass it to the adapt method.\n",
    "\n",
    "> Tip: apply a larger batch size to this reader to speed up performance.  The only calculations being made are the mean and variance and they are only computed once, prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7d96f08-9586-4c95-8955-d48ac8189674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 30), dtype=float32, numpy=\n",
       " array([[ 9.4811133e+04, -2.1519326e-04,  3.1602196e-04, -5.2488595e-04,\n",
       "          6.9466559e-04, -1.2641819e-03,  2.0892750e-03, -7.2106207e-04,\n",
       "         -1.0636140e-03,  1.4059977e-03, -7.1558170e-05, -6.4140302e-04,\n",
       "         -1.5961546e-03,  1.8235012e-03, -6.6740625e-04,  4.2201288e-04,\n",
       "         -2.3144973e-04,  5.9940410e-04, -7.0123409e-04, -1.1209438e-03,\n",
       "          7.4361498e-04, -5.4229691e-04,  7.6822005e-04,  3.2623837e-04,\n",
       "          3.5052517e-04, -5.9398869e-04,  4.6557584e-04, -6.2941969e-04,\n",
       "         -8.2514744e-05,  8.8535156e+01]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 30), dtype=float32, numpy=\n",
       " array([[2.2556255e+09, 3.8344266e+00, 2.7213738e+00, 2.3109169e+00,\n",
       "         2.0030899e+00, 1.9093831e+00, 1.7799078e+00, 1.5511755e+00,\n",
       "         1.4520742e+00, 1.2102603e+00, 1.1956733e+00, 1.0426062e+00,\n",
       "         1.0046861e+00, 9.9211246e-01, 9.2476696e-01, 8.3907151e-01,\n",
       "         7.6823246e-01, 7.2538882e-01, 7.0431167e-01, 6.6373295e-01,\n",
       "         5.9766579e-01, 5.4661345e-01, 5.2690613e-01, 3.7746012e-01,\n",
       "         3.6725327e-01, 2.7222067e-01, 2.3230758e-01, 1.6414191e-01,\n",
       "         1.0881755e-01, 6.3182051e+04]], dtype=float32)>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_array_reader = training_reader.map(fn1).map(lambda v: v.pop('feature_array'))\n",
    "normalizer = tf.keras.layers.Normalization(name = 'normalize', axis = -1)\n",
    "normalizer.adapt(feature_array_reader.prefetch(2).batch(10000))\n",
    "normalizer.mean, normalizer.variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931e006-4a56-416d-bad5-ce473f7bb4cf",
   "metadata": {},
   "source": [
    "Similarly, build a de-normalizer to help return final reconstructed values from the autoencoder to the original scale.  This also uses [tf.keras.layers.Normalization()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) but the mean and variance calculated for the normalization layer can be directly input while also using the `invert = True` argument to indicate an inverse transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35c8d980-60e2-4744-a0c7-ac80ed9c938c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "denormalizer = tf.keras.layers.Normalization(\n",
    "    name = 'denormalize',\n",
    "    mean = normalizer.mean,\n",
    "    variance = normalizer.variance,\n",
    "    invert = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac81c22-9833-4995-867b-08d3ce74f35d",
   "metadata": {},
   "source": [
    "Build the model with layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2df1cc81-0b4e-4ba3-815f-e20b9ba4910d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature inputs: autoencoder\n",
    "feature_inputs = [tf.keras.Input(shape = (1,), dtype = dtypes.float64, name = feature) for feature in training_data.columns]\n",
    "\n",
    "# input layer of concatenated features\n",
    "feature_layer = tf.keras.layers.Concatenate(name = 'feature_layer')(feature_inputs)\n",
    "\n",
    "# use pre-learned normalizer a layer in model\n",
    "norm_layer = normalizer(feature_layer)\n",
    "\n",
    "# encoder\n",
    "encoder = tf.keras.layers.Dense(128, activation = tf.nn.relu)(norm_layer)\n",
    "encoder = tf.keras.layers.Dense(64, activation = tf.nn.relu)(encoder)\n",
    "encoder = tf.keras.layers.Dense(8, activation = tf.nn.relu, name = 'encoder')(encoder)\n",
    "\n",
    "# decoder\n",
    "decoder = tf.keras.layers.Dense(64, activation = tf.nn.relu)(encoder)\n",
    "decoder = tf.keras.layers.Dense(128, activation = tf.nn.relu)(decoder)\n",
    "decoder = tf.keras.layers.Dense(feature_layer.shape[1], activation = tf.nn.sigmoid, name = 'decoder')(decoder)\n",
    "\n",
    "# de-normalize \n",
    "reconstruct = denormalizer(decoder)\n",
    "\n",
    "# define loss function - custom\n",
    "def mae_loss(norm_layer, decoder):\n",
    "    return tf.keras.losses.mae(norm_layer, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8b52f-6d45-4117-92da-47259b950fbb",
   "metadata": {},
   "source": [
    "Create a model from the layers using [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model).  The inputs will be the first layer, 'feature_inputs' and the outputs will each of: encoder, decoder, and reconstruct layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d12ed68-a0ab-41c1-95ad-8483fe30824e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(\n",
    "    inputs = feature_inputs,\n",
    "    outputs = {\n",
    "        'feature_layer': feature_layer,\n",
    "        'norm_layer': norm_layer,\n",
    "        'decoder': decoder,\n",
    "        'reconstruct': reconstruct\n",
    "    },\n",
    "    name = 'autoencoder_from_dataframe'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97227245-3610-4190-8f29-06d52da91223",
   "metadata": {},
   "source": [
    "Compile the model to make it ready for training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "362dfd4d-e037-4632-b230-0c0b2b8f45e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(), #SGD or Adam\n",
    "    loss = {'decoder': tf.keras.losses.MeanAbsoluteError()},\n",
    "    #loss = {'decoder': mae_loss},\n",
    "    metrics = {'decoder': [\n",
    "        tf.keras.metrics.RootMeanSquaredError(name = 'rmse'),\n",
    "        tf.keras.metrics.MeanSquaredError(name = 'mse'),\n",
    "        tf.keras.metrics.MeanAbsoluteError(name = 'mae'),\n",
    "        tf.keras.metrics.MeanSquaredLogarithmicError(name = 'msle'),\n",
    "    ]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3158054f-0799-4f4a-8bea-73fddef8e433",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2281/2281 [==============================] - 15s 6ms/step - loss: 3163.7671 - decoder_loss: 3163.7671 - decoder_rmse: 19360.3203 - decoder_mse: 374821568.0000 - decoder_mae: 3163.7671 - decoder_msle: 4.1063\n",
      "Epoch 2/2\n",
      "2281/2281 [==============================] - 13s 6ms/step - loss: 3163.7390 - decoder_loss: 3163.7390 - decoder_rmse: 19360.3203 - decoder_mse: 374822112.0000 - decoder_mae: 3163.7390 - decoder_msle: 4.0862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc7b7b4ceb0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    training_reader.prefetch(2).map(fn1).map(lambda v: (v, v.pop('feature_array'))).shuffle(1000).batch(100),\n",
    "    epochs = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422625a-3867-4454-8b91-643c76d3cd2f",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd5444-7646-4ead-9e4b-c0c8c76ff216",
   "metadata": {},
   "source": [
    "Retrieve a record, also called an instance, to use for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b33d2ba-fac0-4a75-8a39-395018c53b36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Time': array([2812]),\n",
       " 'V1': array([-0.63340299]),\n",
       " 'V2': array([0.96361604]),\n",
       " 'V3': array([2.49494562]),\n",
       " 'V4': array([2.09905099]),\n",
       " 'V5': array([-0.40433067]),\n",
       " 'V6': array([0.23586158]),\n",
       " 'V7': array([-0.00793191]),\n",
       " 'V8': array([0.21144152]),\n",
       " 'V9': array([-0.20981682]),\n",
       " 'V10': array([0.3082976]),\n",
       " 'V11': array([-1.20499231]),\n",
       " 'V12': array([-0.47470781]),\n",
       " 'V13': array([-0.65406356]),\n",
       " 'V14': array([-0.47459911]),\n",
       " 'V15': array([-0.42841779]),\n",
       " 'V16': array([0.53665148]),\n",
       " 'V17': array([-0.38065462]),\n",
       " 'V18': array([0.02865054]),\n",
       " 'V19': array([-0.68796943]),\n",
       " 'V20': array([-0.17498476]),\n",
       " 'V21': array([0.01467553]),\n",
       " 'V22': array([0.01627818]),\n",
       " 'V23': array([-0.06146247]),\n",
       " 'V24': array([0.35519634]),\n",
       " 'V25': array([-0.1790855]),\n",
       " 'V26': array([-0.10694743]),\n",
       " 'V27': array([-0.21503926]),\n",
       " 'V28': array([0.0506978]),\n",
       " 'Amount': array([0.])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_iter = iter(training_reader.batch(1).take(1))\n",
    "instance = {key: value.numpy() for key, value in next(ds_iter).items()}\n",
    "instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707578dc-eab6-4361-b9a7-683c6e6574ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "Get the prediction for the instance directly from the model using the [predict()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "301e5965-62b5-4d02-949c-f26fcb53a61d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'feature_layer': array([[ 2.8120000e+03, -6.3340300e-01,  9.6361601e-01,  2.4949455e+00,\n",
       "          2.0990510e+00, -4.0433067e-01,  2.3586158e-01, -7.9319049e-03,\n",
       "          2.1144152e-01, -2.0981681e-01,  3.0829760e-01, -1.2049923e+00,\n",
       "         -4.7470781e-01, -6.5406358e-01, -4.7459912e-01, -4.2841780e-01,\n",
       "          5.3665149e-01, -3.8065460e-01,  2.8650539e-02, -6.8796945e-01,\n",
       "         -1.7498475e-01,  1.4675528e-02,  1.6278177e-02, -6.1462473e-02,\n",
       "          3.5519636e-01, -1.7908551e-01, -1.0694742e-01, -2.1503925e-01,\n",
       "          5.0697796e-02,  0.0000000e+00]], dtype=float32),\n",
       " 'norm_layer': array([[-1.937092  , -0.32335705,  0.58393896,  1.6415733 ,  1.482617  ,\n",
       "         -0.2916958 ,  0.17522429, -0.00578969,  0.17634982, -0.19200009,\n",
       "          0.28201008, -1.1794863 , -0.472007  , -0.6584891 , -0.49283284,\n",
       "         -0.4681614 ,  0.61253834, -0.44764012,  0.03497453, -0.8430712 ,\n",
       "         -0.22730693,  0.02058318,  0.02136702, -0.10057119,  0.5855404 ,\n",
       "         -0.34210312, -0.22285642, -0.52921844,  0.15393801, -0.35222396]],\n",
       "       dtype=float32),\n",
       " 'decoder': array([[1.0000000e+00, 8.7562431e-12, 9.0617776e-01, 9.9999970e-01,\n",
       "         9.9729848e-01, 5.3546061e-03, 3.5449871e-01, 4.9829915e-02,\n",
       "         9.8819606e-02, 3.1882478e-03, 8.7474465e-02, 5.2347924e-03,\n",
       "         1.5399154e-01, 3.9444816e-09, 8.4945525e-04, 3.6342286e-03,\n",
       "         1.8889314e-11, 3.1541131e-02, 1.0524126e-01, 8.1478614e-01,\n",
       "         8.0608316e-02, 2.9843172e-02, 1.8696433e-01, 8.2345781e-05,\n",
       "         2.7546874e-01, 4.6859615e-05, 1.4847802e-09, 7.3626056e-02,\n",
       "         6.6265360e-02, 1.0000000e+00]], dtype=float32),\n",
       " 'reconstruct': array([[ 1.4230456e+05, -2.1519324e-04,  1.4952000e+00,  1.5196446e+00,\n",
       "          1.4121767e+00,  6.1348355e-03,  4.7503728e-01,  6.1340198e-02,\n",
       "          1.1801603e-01,  4.9134474e-03,  9.5579021e-02,  4.7037434e-03,\n",
       "          1.5275577e-01,  1.8235052e-03,  1.4947075e-04,  3.7509971e-03,\n",
       "         -2.3144971e-04,  2.7462911e-02,  8.7620683e-02,  6.6268378e-01,\n",
       "          6.3060969e-02,  2.1521749e-02,  1.3648245e-01,  3.7682979e-04,\n",
       "          1.6728850e-01, -5.6953978e-04,  4.6557657e-04,  2.9199759e-02,\n",
       "          2.1776773e-02,  3.3989557e+02]], dtype=float32)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(instance)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557aae7-e9d8-4c89-90ef-a48d34030ea8",
   "metadata": {},
   "source": [
    "The model has named outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13dc6c74-c408-442d-b4d6-c59b987253ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 30) dtype=float32 (created by layer 'decoder')>,\n",
       " <KerasTensor: shape=(None, 30) dtype=float32 (created by layer 'feature_layer')>,\n",
       " <KerasTensor: shape=(None, 30) dtype=float32 (created by layer 'normalize')>,\n",
       " <KerasTensor: shape=(None, 30) dtype=float32 (created by layer 'denormalize')>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536042d8-4172-4a42-9765-0812b16fd657",
   "metadata": {},
   "source": [
    "Which can be directly referenced in the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8328dfd4-6824-439f-b0e1-f5fe70253c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.4230456e+05, -2.1519324e-04,  1.4952000e+00,  1.5196446e+00,\n",
       "         1.4121767e+00,  6.1348355e-03,  4.7503728e-01,  6.1340198e-02,\n",
       "         1.1801603e-01,  4.9134474e-03,  9.5579021e-02,  4.7037434e-03,\n",
       "         1.5275577e-01,  1.8235052e-03,  1.4947075e-04,  3.7509971e-03,\n",
       "        -2.3144971e-04,  2.7462911e-02,  8.7620683e-02,  6.6268378e-01,\n",
       "         6.3060969e-02,  2.1521749e-02,  1.3648245e-01,  3.7682979e-04,\n",
       "         1.6728850e-01, -5.6953978e-04,  4.6557657e-04,  2.9199759e-02,\n",
       "         2.1776773e-02,  3.3989557e+02]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction['reconstruct']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c75078-6e84-4429-8108-e7aa25a3fcdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Multiple records, also called instances, can be predicted at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2d4399e-7185-47d4-945d-225fafcf0efb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Time': array([2812, 3150]),\n",
       " 'V1': array([-0.63340299,  1.31328087]),\n",
       " 'V2': array([ 0.96361604, -0.25792282]),\n",
       " 'V3': array([2.49494562, 0.11846283]),\n",
       " 'V4': array([ 2.09905099, -0.73555665]),\n",
       " 'V5': array([-0.40433067, -0.56930772]),\n",
       " 'V6': array([ 0.23586158, -0.73357721]),\n",
       " 'V7': array([-0.00793191, -0.13865918]),\n",
       " 'V8': array([ 0.21144152, -0.14164134]),\n",
       " 'V9': array([-0.20981682,  1.70801916]),\n",
       " 'V10': array([ 0.3082976 , -1.10329377]),\n",
       " 'V11': array([-1.20499231, -1.08782009]),\n",
       " 'V12': array([-0.47470781,  0.64467588]),\n",
       " 'V13': array([-0.65406356, -0.21536864]),\n",
       " 'V14': array([-0.47459911, -0.07471497]),\n",
       " 'V15': array([-0.42841779,  0.28787333]),\n",
       " 'V16': array([ 0.53665148, -1.00176397]),\n",
       " 'V17': array([-0.38065462,  0.09376776]),\n",
       " 'V18': array([ 0.02865054, -0.07254906]),\n",
       " 'V19': array([-0.68796943,  1.1083853 ]),\n",
       " 'V20': array([-0.17498476, -0.14514383]),\n",
       " 'V21': array([ 0.01467553, -0.08246737]),\n",
       " 'V22': array([0.01627818, 0.12606591]),\n",
       " 'V23': array([-0.06146247, -0.22315725]),\n",
       " 'V24': array([ 0.35519634, -0.07497714]),\n",
       " 'V25': array([-0.1790855 ,  0.92193958]),\n",
       " 'V26': array([-0.10694743, -0.52828348]),\n",
       " 'V27': array([-0.21503926,  0.06447566]),\n",
       " 'V28': array([0.0506978 , 0.01313189]),\n",
       " 'Amount': array([0., 0.])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_iter = iter(training_reader.batch(2).take(1))\n",
    "instances = {key: value.numpy() for key, value in next(ds_iter).items()}\n",
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "25008a2c-0c38-405a-bb3f-472ba7f5b8bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.4230456e+05, -2.1519324e-04,  1.4951999e+00,  1.5196446e+00,\n",
       "         1.4121767e+00,  6.1348290e-03,  4.7503763e-01,  6.1340131e-02,\n",
       "         1.1801631e-01,  4.9134521e-03,  9.5578760e-02,  4.7037387e-03,\n",
       "         1.5275575e-01,  1.8235052e-03,  1.4947308e-04,  3.7510036e-03,\n",
       "        -2.3144971e-04,  2.7462857e-02,  8.7620832e-02,  6.6268361e-01,\n",
       "         6.3060932e-02,  2.1521749e-02,  1.3648215e-01,  3.7682979e-04,\n",
       "         1.6728833e-01, -5.6953978e-04,  4.6557657e-04,  2.9199719e-02,\n",
       "         2.1776769e-02,  3.3989557e+02],\n",
       "       [ 1.4230456e+05,  1.9579538e+00,  3.1602365e-04,  1.6818026e-01,\n",
       "         5.4540168e-02, -1.2403572e-03,  2.9143826e-03,  4.6561495e-04,\n",
       "         2.7090572e-02,  1.0978662e+00, -7.0910319e-05, -3.6981062e-04,\n",
       "         6.6006875e-01,  1.8235403e-03,  2.9768083e-02,  3.6315274e-01,\n",
       "        -2.3123699e-04,  2.2773892e-02,  8.3688293e-03, -1.1208523e-03,\n",
       "         7.4997096e-04,  3.4153730e-02,  1.8506378e-01,  3.2702286e-04,\n",
       "         1.5659017e-02,  1.4736524e-02,  4.6563719e-04, -5.2936428e-04,\n",
       "         1.3674238e-03,  3.3989478e+02]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(instances)\n",
    "prediction['reconstruct']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62a32c-d48d-4eb5-bf5f-3ed4a6657ede",
   "metadata": {},
   "source": [
    "The input side of the model was built to take named inputs, columns.  An advantage of this is that the inputs at prediction time can change order and the model will reassemble them in the correct order for inference.  Had the model been trained on an array of feature values, then the array would need to be provide here at inference time in the exact same order.  This can be very helpful when inputs are images with nature order of features: rows, columns, and layers of pixels.  For tabular data arranged in named columns, using named inputs prevent training/serving skew by removing the need to both gather the input columns and order them.\n",
    "\n",
    "To demonstrate, the single instance above will be pass in the correct and in reversed order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8fbed97-e88a-4ca1-bf5e-0aae35528fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Time': array([2812]),\n",
       " 'V1': array([-0.63340299]),\n",
       " 'V2': array([0.96361604]),\n",
       " 'V3': array([2.49494562]),\n",
       " 'V4': array([2.09905099]),\n",
       " 'V5': array([-0.40433067]),\n",
       " 'V6': array([0.23586158]),\n",
       " 'V7': array([-0.00793191]),\n",
       " 'V8': array([0.21144152]),\n",
       " 'V9': array([-0.20981682]),\n",
       " 'V10': array([0.3082976]),\n",
       " 'V11': array([-1.20499231]),\n",
       " 'V12': array([-0.47470781]),\n",
       " 'V13': array([-0.65406356]),\n",
       " 'V14': array([-0.47459911]),\n",
       " 'V15': array([-0.42841779]),\n",
       " 'V16': array([0.53665148]),\n",
       " 'V17': array([-0.38065462]),\n",
       " 'V18': array([0.02865054]),\n",
       " 'V19': array([-0.68796943]),\n",
       " 'V20': array([-0.17498476]),\n",
       " 'V21': array([0.01467553]),\n",
       " 'V22': array([0.01627818]),\n",
       " 'V23': array([-0.06146247]),\n",
       " 'V24': array([0.35519634]),\n",
       " 'V25': array([-0.1790855]),\n",
       " 'V26': array([-0.10694743]),\n",
       " 'V27': array([-0.21503926]),\n",
       " 'V28': array([0.0506978]),\n",
       " 'Amount': array([0.])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fa0cc515-2c04-4f1e-a0de-f29593a9066e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Amount': array([0.]),\n",
       " 'V28': array([0.0506978]),\n",
       " 'V27': array([-0.21503926]),\n",
       " 'V26': array([-0.10694743]),\n",
       " 'V25': array([-0.1790855]),\n",
       " 'V24': array([0.35519634]),\n",
       " 'V23': array([-0.06146247]),\n",
       " 'V22': array([0.01627818]),\n",
       " 'V21': array([0.01467553]),\n",
       " 'V20': array([-0.17498476]),\n",
       " 'V19': array([-0.68796943]),\n",
       " 'V18': array([0.02865054]),\n",
       " 'V17': array([-0.38065462]),\n",
       " 'V16': array([0.53665148]),\n",
       " 'V15': array([-0.42841779]),\n",
       " 'V14': array([-0.47459911]),\n",
       " 'V13': array([-0.65406356]),\n",
       " 'V12': array([-0.47470781]),\n",
       " 'V11': array([-1.20499231]),\n",
       " 'V10': array([0.3082976]),\n",
       " 'V9': array([-0.20981682]),\n",
       " 'V8': array([0.21144152]),\n",
       " 'V7': array([-0.00793191]),\n",
       " 'V6': array([0.23586158]),\n",
       " 'V5': array([-0.40433067]),\n",
       " 'V4': array([2.09905099]),\n",
       " 'V3': array([2.49494562]),\n",
       " 'V2': array([0.96361604]),\n",
       " 'V1': array([-0.63340299]),\n",
       " 'Time': array([2812])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_reversed = {k:v for k,v in reversed(instance.items())}\n",
    "instance_reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca7e2804-a85c-493e-b0b7-6d26b34b9e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.4230456e+05, -2.1519324e-04,  1.4952000e+00,  1.5196446e+00,\n",
       "          1.4121767e+00,  6.1348355e-03,  4.7503728e-01,  6.1340198e-02,\n",
       "          1.1801603e-01,  4.9134474e-03,  9.5579021e-02,  4.7037434e-03,\n",
       "          1.5275577e-01,  1.8235052e-03,  1.4947075e-04,  3.7509971e-03,\n",
       "         -2.3144971e-04,  2.7462911e-02,  8.7620683e-02,  6.6268378e-01,\n",
       "          6.3060969e-02,  2.1521749e-02,  1.3648245e-01,  3.7682979e-04,\n",
       "          1.6728850e-01, -5.6953978e-04,  4.6557657e-04,  2.9199759e-02,\n",
       "          2.1776773e-02,  3.3989557e+02]], dtype=float32),\n",
       " array([[ 1.4230456e+05, -2.1519324e-04,  1.4952000e+00,  1.5196446e+00,\n",
       "          1.4121767e+00,  6.1348355e-03,  4.7503728e-01,  6.1340198e-02,\n",
       "          1.1801603e-01,  4.9134474e-03,  9.5579021e-02,  4.7037434e-03,\n",
       "          1.5275577e-01,  1.8235052e-03,  1.4947075e-04,  3.7509971e-03,\n",
       "         -2.3144971e-04,  2.7462911e-02,  8.7620683e-02,  6.6268378e-01,\n",
       "          6.3060969e-02,  2.1521749e-02,  1.3648245e-01,  3.7682979e-04,\n",
       "          1.6728850e-01, -5.6953978e-04,  4.6557657e-04,  2.9199759e-02,\n",
       "          2.1776773e-02,  3.3989557e+02]], dtype=float32))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model.predict(instance)['reconstruct']\n",
    "prediction_reversed = model.predict(instance_reversed)['reconstruct']\n",
    "prediction, prediction_reversed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b4658-a463-47c0-a38f-6c3a9936d9ef",
   "metadata": {},
   "source": [
    "Both prediction look identical even thought the instance was input in a different order.  This can also be programatically checked for confirmation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "87f20ca2-bc4f-4cdc-b129-8a0c05100680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prediction == prediction_reversed).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de56fe-5654-47d4-bd4f-4560a1132b34",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "\n",
    "An incredible feature of TensorFlow/Keras is being able to treat models as layers. \n",
    "\n",
    "The trained model above outputs the reconstructed feature array from the trained autoencoder.  It would be great to have more information and different formatting though. Like:\n",
    "- report instance level metrics to help interpret the reconstruction: MAE, MSE, MSLE\n",
    "    - base these on the actual values and the normalized values\n",
    "- return the reconstructed feature array to named elements matching the named inputs to the model\n",
    "- order the reconstructed feature array in decending order by magnitude of the error\n",
    "    - base magnitude of error on the absolute difference in normalized values\n",
    "    \n",
    "The following builds post-processing model that takes the outputs of the autoencoder as inputs.  Then, a new model is built that combines these together into a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "97976761-e8ac-42a2-8b7b-8f50ed0b4278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# metric calcs on denormalized values\n",
    "mean_absolute_error = tf.keras.losses.mae(reconstruct, feature_layer)\n",
    "mean_squared_error = tf.keras.losses.mse(reconstruct, feature_layer)\n",
    "mean_squared_log_error = tf.keras.losses.msle(reconstruct, feature_layer)\n",
    "\n",
    "# metric calc on normalized values\n",
    "norm_mean_absolute_error = tf.keras.losses.mae(norm_layer, decoder)\n",
    "norm_mean_squared_error = tf.keras.losses.mse(norm_layer, decoder)\n",
    "norm_mean_squared_log_error = tf.keras.losses.msle(norm_layer, decoder)\n",
    "\n",
    "# list reconstruction error for each feature\n",
    "errors = [{feature_inputs[v].name : val} for v, val in enumerate(reconstruct[0,:])]\n",
    "\n",
    "# errors ordered by norm error absolute magnitude\n",
    "norm_abs_diffs = tf.math.abs(norm_layer - decoder)\n",
    "ordered_norm_abs_diffs = tf.argsort(norm_abs_diffs, direction = 'DESCENDING')\n",
    "errors_impact_order = [i for i in ordered_norm_abs_diffs[0,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "91b8dc90-9db3-4eb1-9839-7fe91d1aee87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post_model = tf.keras.Model(\n",
    "    #inputs = {k: v for k, v in model.output.items() if k in ['reconstruct', 'feature_layer']},\n",
    "    inputs = model.output,\n",
    "    outputs = {\n",
    "        'mean_absolute_error': mean_absolute_error[0],\n",
    "        'mean_squared_error': mean_squared_error[0],\n",
    "        'mean_squared_log_error': mean_squared_log_error[0],\n",
    "        'norm_mean_absolute_error': norm_mean_absolute_error[0],\n",
    "        'norm_mean_squared_error': norm_mean_squared_error[0],\n",
    "        'norm_mean_squared_log_error': norm_mean_squared_log_error[0],\n",
    "        'errors': errors,\n",
    "        'errors_impact_order': errors_impact_order\n",
    "    },\n",
    "    name = 'autoencoder_post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ddef82d3-a174-42bf-b273-cb3d3b6da6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_model = tf.keras.Model(\n",
    "    inputs = model.inputs,\n",
    "    outputs = post_model(model(model.inputs))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4849ca75-ab48-41ba-8834-40fbddea8770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_absolute_error': 4661.44775390625,\n",
       " 'mean_squared_error': 648609664.0,\n",
       " 'mean_squared_log_error': 1.67367684841156,\n",
       " 'norm_mean_absolute_error': 0.5154851078987122,\n",
       " 'norm_mean_squared_error': 0.6112011671066284,\n",
       " 'norm_mean_squared_log_error': 0.06236143782734871,\n",
       " 'errors': [{'Time': 142304.5625},\n",
       "  {'V1': -0.00021519324218388647},\n",
       "  {'V2': 1.4952000379562378},\n",
       "  {'V3': 1.5196446180343628},\n",
       "  {'V4': 1.4121767282485962},\n",
       "  {'V5': 0.006134835537523031},\n",
       "  {'V6': 0.47503727674484253},\n",
       "  {'V7': 0.061340197920799255},\n",
       "  {'V8': 0.11801602691411972},\n",
       "  {'V9': 0.0049134474247694016},\n",
       "  {'V10': 0.09557902067899704},\n",
       "  {'V11': 0.0047037433832883835},\n",
       "  {'V12': 0.1527557671070099},\n",
       "  {'V13': 0.0018235051538795233},\n",
       "  {'V14': 0.000149470753967762},\n",
       "  {'V15': 0.003750997129827738},\n",
       "  {'V16': -0.0002314497105544433},\n",
       "  {'V17': 0.0274629108607769},\n",
       "  {'V18': 0.08762068301439285},\n",
       "  {'V19': 0.6626837849617004},\n",
       "  {'V20': 0.06306096911430359},\n",
       "  {'V21': 0.02152174897491932},\n",
       "  {'V22': 0.13648244738578796},\n",
       "  {'V23': 0.0003768297901842743},\n",
       "  {'V24': 0.16728849709033966},\n",
       "  {'V25': -0.0005695397849194705},\n",
       "  {'V26': 0.0004655765660572797},\n",
       "  {'V27': 0.029199758544564247},\n",
       "  {'V28': 0.021776773035526276},\n",
       "  {'Amount': 339.89556884765625}],\n",
       " 'errors_impact_order': [0,\n",
       "  19,\n",
       "  29,\n",
       "  11,\n",
       "  13,\n",
       "  3,\n",
       "  12,\n",
       "  16,\n",
       "  27,\n",
       "  14,\n",
       "  4,\n",
       "  17,\n",
       "  15,\n",
       "  25,\n",
       "  1,\n",
       "  2,\n",
       "  24,\n",
       "  20,\n",
       "  5,\n",
       "  26,\n",
       "  9,\n",
       "  10,\n",
       "  6,\n",
       "  22,\n",
       "  23,\n",
       "  28,\n",
       "  8,\n",
       "  18,\n",
       "  7,\n",
       "  21]}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.predict(instance)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
